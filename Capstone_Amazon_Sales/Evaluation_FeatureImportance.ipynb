{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IruZsj1v2ukt"
   },
   "source": [
    "# Evaluation - Feature Importance \n",
    "\n",
    "### Objectives:\n",
    "- Uncover the factors that significantly impact sales\n",
    "- Build a machine learning model with a 90% accuracy rating (less than 10% average error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Functions Used</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_file():\n",
    "    app = QApplication([])\n",
    "    file_path, _ = QFileDialog.getOpenFileName(None, \n",
    "                                               \"Select a CSV or XLSX file\", \"\", \n",
    "                                               \"Files (*.xlsx *.csv)\")\n",
    "    \n",
    "    if not file_path:  # If no file is selected\n",
    "        print(\"No file selected.\")\n",
    "        return None\n",
    "    \n",
    "    if file_path.endswith('.xlsx'):\n",
    "        print(\"XLSX file selected. Converting to CSV.\")\n",
    "        df = pd.read_excel(file_path)\n",
    "        csv_path = file_path.rsplit('.', 1)[0] + '.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"CSV file created: {csv_path}\")\n",
    "    elif file_path.endswith('.csv'):\n",
    "        print(\"CSV file selected. Reading file.\")\n",
    "        csv_path = file_path\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type selected.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    \n",
    "    # Define the columns needed for analysis to simplify the dataset.\n",
    "    cols_needed = ['Date', 'Category', 'Style', 'Size', 'SKU', 'Qty', 'Amount', 'promotion-ids', 'B2B']\n",
    "    \n",
    "    # Remove duplicates based on 'Order ID' to ensure unique transactions and select only the necessary columns.\n",
    "    data = df.drop_duplicates(subset='Order ID')[cols_needed]\n",
    "    \n",
    "    # Standardize column names by renaming them to maintain consistent naming conventions across the dataset.\n",
    "    data.rename(columns={'Qty': 'Quantity', 'promotion-ids': 'Promotions'}, inplace=True)\n",
    "    \n",
    "    # Fill missing values in 'Promotions' column with \"No Promotion\" to indicate transactions without any promotions.\n",
    "    data['Promotions'].fillna(\"No Promotion\", inplace=True)\n",
    "    \n",
    "    # Remove transactions with missing 'Amount' values as these represent incomplete data points.\n",
    "    data.dropna(subset=['Amount'], inplace=True)\n",
    "    data = data[(data['Quantity'] > 0) & (data['Amount'] > 0)]\n",
    "    \n",
    "    # Convert 'Amount' from INR to USD for standardization, assuming a fixed exchange rate (1 INR = 0.012 USD).\n",
    "    data['Amount'] = data['Amount'] * 0.012\n",
    "    \n",
    "    # Simplify 'Promotions' column\n",
    "    # Extract relevant information from Promotions\n",
    "    promotions = [\"Free-Financing\", \"Free Shipping\", \"Duplicated\", \"Coupon\"]\n",
    "    data['Promotions'] = data['Promotions'].apply(lambda x: next((promo for promo in promotions if promo in str(x)), x))\n",
    "    \n",
    "    # Convert \"B2B\" boolean to integer\n",
    "    data['B2B'] = data['B2B'].astype(int)\n",
    "    \n",
    "    # Adjust 'Size' values\n",
    "    data['Size'] = data['Size'].replace({'Free': 'Any'})\n",
    "\n",
    "    # Convert the 'Date' column from string to DateTime objects for easier manipulation and analysis.\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    \n",
    "    # Remove numbers from 'Style'\n",
    "    data['Style'] = data['Style'].str.replace('\\d+', '', regex=True)\n",
    "    \n",
    "    # Sort by 'Date'\n",
    "    data_sorted = data.sort_values(by='Date', ascending=True)\n",
    "    \n",
    "    # Return the cleaned dataset.\n",
    "    return data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \n",
    "    # Extract colors from 'SKU'\n",
    "    color_names = ['MUSTARD', 'BLUE', 'NAVY', 'WHITE', 'BROWN', 'GREEN', 'BIEGE', \n",
    "                   'RED', 'PURPLE', 'MAROON', 'PINK', 'CHIKU', 'BLACK', 'GOLD', \n",
    "                   'BEIGE', 'ORANGE', 'YELLOW']\n",
    "    \n",
    "    # Extract the Colors from the column SKU\n",
    "    df['SKU_Color'] = df['SKU'].apply(lambda sku: next((color for color in color_names if color in sku), 'NOT LISTED'))\n",
    "\n",
    "    # Extracting Date (Year, Month, and Days)\n",
    "#     data['Year'] = data['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "\n",
    "    # Dropping features that have extracted information\n",
    "    df = df.drop(['Date', 'SKU'], axis=1).reset_index(drop=True)\n",
    "        \n",
    "    # Return the engineered dataset.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    X = df.drop('Amount', axis=1)\n",
    "    y = df['Amount']\n",
    "\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(X_train, X_test):\n",
    "    # Columns for one-hot encoding and label encoding\n",
    "    ohe_cols = X_train.select_dtypes(include='object').columns.to_list()\n",
    "    scale_cols = ['Quantity', 'Month', 'Day']\n",
    "    \n",
    "    # Preprocessor for transformations\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ohe_cols),\n",
    "            ('scale', MinMaxScaler(), scale_cols)\n",
    "        ], remainder='passthrough')  # 'B2B' remains unchanged\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    X_train_transformed = pd.DataFrame(X_train_transformed, columns = preprocessor.get_feature_names_out())\n",
    "    X_test_transformed = pd.DataFrame(X_test_transformed, columns = preprocessor.get_feature_names_out())\n",
    "    \n",
    "    return X_train_transformed, X_test_transformed, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation(target):\n",
    "    \"\"\"Transform the target variable to reduce skewness.\"\"\"\n",
    "    transformations = {\n",
    "        \"sqrt\": np.sqrt(target.clip(lower=0)),\n",
    "        \"p1_3\": target**(1/3),\n",
    "        \"p1_4\": target**(1/4),\n",
    "        \"p1_5\": target**(1/5),\n",
    "        \"log1p\": np.log1p(target)\n",
    "    }\n",
    "    skewness = {key: val.skew() for key, val in transformations.items()}\n",
    "    best_transformation = min(skewness, key=skewness.get)\n",
    "    print(f'Best transformation is \"{best_transformation}\" with skewness {skewness[best_transformation]}')\n",
    "    return transformations[best_transformation], best_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(predictions, transformations):\n",
    "    if 'log1p' in transformations:\n",
    "        return np.expm1(predictions)\n",
    "    elif 'sqrt' in transformations:\n",
    "        return np.power(predictions, 2)\n",
    "    else:\n",
    "        print('No transformation needed!')\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, transformations):\n",
    "    model_name = model.__class__.__name__\n",
    "    print(f\"Scoring {model_name}\")\n",
    "    \n",
    "    \n",
    "    # Fit the model\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    endtime = time.time() - start\n",
    "    \n",
    "    # Predicting\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform the predictions and true values only once\n",
    "    y_train_original, y_test_original = inverse_transform(y_train, transformations), inverse_transform(y_test, transformations)\n",
    "    y_pred_train_original, y_pred_test_original = inverse_transform(y_pred_train, transformations), inverse_transform(y_pred_test, transformations)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'RMSE Train Set': np.sqrt(mean_squared_error(y_train_original, y_pred_train_original)),\n",
    "        'RMSE Test Set': np.sqrt(mean_squared_error(y_test_original, y_pred_test_original)),\n",
    "        'MAE Train Set': mean_absolute_error(y_train_original, y_pred_train_original),\n",
    "        'MAE Test Set': mean_absolute_error(y_test_original, y_pred_test_original),\n",
    "        'R2 Train Set': r2_score(y_train_original, y_pred_train_original),\n",
    "        'R2 Test Set': r2_score(y_test_original, y_pred_test_original),\n",
    "        'Time(sec)': endtime\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_for_best_model(grid_models, param_dict, scoring_dict, X_train, y_train):\n",
    "\n",
    "    # Container for grid search results\n",
    "    grid_results = []\n",
    "\n",
    "    # Running grid search for each model\n",
    "    for model_name, model in grid_models.items():\n",
    "        print(f\"Running GridSearchCV for {model_name}...\")\n",
    "        start = time.time()\n",
    "        grid = GridSearchCV(model, param_grid=param_dict[model_name], cv=5, verbose=False,\n",
    "                            scoring=scoring_dict, refit='MAE')\n",
    "        \n",
    "        grid.fit(X_train, y_train)\n",
    "        fit_time = time.time() - start\n",
    "        \n",
    "        # Extracting best scores\n",
    "        best_score_rmse = np.sqrt(-grid.cv_results_['mean_test_RMSE'].max())\n",
    "        best_score_mae = -grid.cv_results_['mean_test_MAE'].max()\n",
    "        best_score_r2 = grid.cv_results_['mean_test_R2'].max()\n",
    "        \n",
    "        # Appending results\n",
    "        grid_results.append({\n",
    "            'Model': model_name,\n",
    "            'Best Parameters': grid.best_params_,\n",
    "            'RMSE': best_score_rmse,\n",
    "            'MAE': best_score_mae,\n",
    "            'R2': best_score_r2,\n",
    "            'Time(sec)': fit_time\n",
    "        })\n",
    "\n",
    "    # Converting results to DataFrame\n",
    "    results_df = pd.DataFrame(grid_results)\n",
    "    \n",
    "    # Extracting the best model based on RMSE\n",
    "    best_model_info = results_df.loc[results_df['RMSE'].idxmin()]\n",
    "    best_model_name = best_model_info['Model']\n",
    "    best_model = grid_models[best_model_name].set_params(**best_model_info['Best Parameters'])\n",
    "    \n",
    "    print(f\"Best Model: {best_model_name} with parameters {best_model_info['Best Parameters']}\")\n",
    "    \n",
    "    return best_model, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importances_expanded(preprocessor, best_model):\n",
    "        \"\"\"Extracts expanded feature importances from a trained model.\"\"\"\n",
    "        if not hasattr(best_model, 'feature_importances_'):\n",
    "            print(\"The provided model does not have feature_importances_ attribute.\")\n",
    "            return None\n",
    "\n",
    "        imp_list = []\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        feature_importances = best_model.feature_importances_\n",
    "\n",
    "        # Normalize the feature importances\n",
    "        feature_importances_normalized = feature_importances / sum(feature_importances) * 100\n",
    "\n",
    "        # Populate imp_list with feature names and their importances\n",
    "        for name, importance in zip(feature_names, feature_importances_normalized):\n",
    "            imp_list.append({'Importance Name': name, 'Importance(%)': round(importance, 2)})\n",
    "        \n",
    "        return pd.DataFrame(imp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFeatureImportance:\n",
    "    def __init__(self, feature_names, n_repeats=10, random_state=42):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_repeats = n_repeats\n",
    "        self.random_state = random_state\n",
    "        self.feature_importances_data = []\n",
    "\n",
    "    def aggregate_and_normalize_importances(self, importances):\n",
    "        aggregated_importances = defaultdict(float)\n",
    "        for feature, importance in zip(self.feature_names, importances):\n",
    "            base_name = feature.split(\"__\")[1] if \"__\" in feature else feature\n",
    "            base_name = base_name.rsplit(\"_\", 1)[0]\n",
    "            aggregated_importances[base_name] += np.abs(importance)\n",
    "        total_importance = sum(aggregated_importances.values())\n",
    "        return {k: v / total_importance * 100 for k, v in aggregated_importances.items()}\n",
    "\n",
    "    def process_model(self, model, X_test, y_test):\n",
    "        model_name = type(model).__name__\n",
    "        if hasattr(model, 'coef_'):\n",
    "            coefs = model.coef_.flatten()\n",
    "            importances = np.abs(coefs)\n",
    "            importance_type = \"Coefficient Importance\"\n",
    "        elif hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            importance_type = \"Feature Importance\"\n",
    "        else:\n",
    "            result = permutation_importance(model, X_test, y_test, n_repeats=self.n_repeats, random_state=self.random_state)\n",
    "            importances = np.abs(result.importances_mean)\n",
    "            importance_type = \"Permutation Importance\"\n",
    "        \n",
    "        normalized_importances = self.aggregate_and_normalize_importances(importances)\n",
    "        for feature, importance in normalized_importances.items():\n",
    "            self.feature_importances_data.append((model_name, feature, importance, importance_type))\n",
    "\n",
    "    def display_importances(self):\n",
    "        for data in self.feature_importances_data:\n",
    "            model_name, feature, importance, importance_type = data\n",
    "            print(f\"{model_name} - {importance_type} for {feature}: {importance:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_best_model(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on both the training and test sets\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Inverse the data\n",
    "    y_train_original, y_test_original = inverse_transform(y_train, transformations), inverse_transform(y_test, transformations)\n",
    "    y_pred_train, y_pred_test = inverse_transform(y_pred_train, transformations), inverse_transform(y_pred_test, transformations)\n",
    "    \n",
    "    # Combine the actual and predicted values for both training and test sets\n",
    "    actual_combined = np.concatenate((y_train, y_test), axis=0)\n",
    "    predicted_combined = np.concatenate((y_pred_train, y_pred_test), axis=0)\n",
    "    \n",
    "    # Create a DataFrame for the combined actual and predicted values\n",
    "    final_output = pd.DataFrame({\n",
    "        'Actual(USD)': actual_combined,\n",
    "        'Predicted(USD)': predicted_combined\n",
    "    })\n",
    "    \n",
    "    # Calculate errors and differences\n",
    "    final_output['Dollar Difference(USD)'] = round(abs(final_output['Actual(USD)'] - final_output['Predicted(USD)']), 2)\n",
    "    final_output['Error(%)'] = round(100 * final_output['Dollar Difference(USD)'] / final_output['Actual(USD)'], 2)\n",
    "    \n",
    "    # Calculate evaluation metrics for the combined dataset\n",
    "    mape = final_output['Error(%)'].mean()\n",
    "    rmse = np.sqrt(mean_squared_error(final_output['Actual(USD)'], final_output['Predicted(USD)']))\n",
    "    mae = mean_absolute_error(final_output['Actual(USD)'], final_output['Predicted(USD)'])\n",
    "    \n",
    "    return mape, rmse, mae, final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "RAtqBlHh2uky"
   },
   "outputs": [],
   "source": [
    "# INSTALLATIONS\n",
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install numpy\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install category_encoders\n",
    "# !pip install mlxtend\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Imports of Libraries</u>\n",
    "- imports separated for each step along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING & BASIC IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from PyQt5.QtWidgets import QApplication, QFileDialog\n",
    "from collections import defaultdict\n",
    "\n",
    "# PREPROCESSING\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, OrdinalEncoder, PolynomialFeatures, LabelEncoder\n",
    "from category_encoders import TargetEncoder, BinaryEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# MODELING\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "# EVALUATING\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Reading File Into Project</u>\n",
    "- taking into account for both 'xlsx' and 'csv' files\n",
    "- display the full dataset rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file selected. Reading file.\n"
     ]
    }
   ],
   "source": [
    "df = grab_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Data Cleaning</u>\n",
    "- #### Only Keeping Necessary Columns for Analysis\n",
    "- #### Removing Duplicates with Order ID the Same\n",
    "    - False-Positive Transactions\n",
    "- #### Renaming Columns for Similar Naming Conventions\n",
    "- #### Filling missing values in 'promotion-ids' with \"No Promotion\"\n",
    "- #### Drop missing values to disclude any potential biases\n",
    "    - Since 'Amount' is crucial for analysis\n",
    "    - There is only 1 type of currency wich is INR (Indian Rupee) * 0.012 for USD conversion\n",
    "#### OPTIONAL:\n",
    "- #### Extracting Valid Purchases from Orders\n",
    "    - Ordered, Cancelled, Returned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = data_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Category</th>\n",
       "      <th>Style</th>\n",
       "      <th>Size</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Promotions</th>\n",
       "      <th>B2B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48972</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>Set</td>\n",
       "      <td>J</td>\n",
       "      <td>XS</td>\n",
       "      <td>J0127-SKD-XS</td>\n",
       "      <td>1</td>\n",
       "      <td>14.388</td>\n",
       "      <td>Free Shipping</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49017</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>kurta</td>\n",
       "      <td>JNE</td>\n",
       "      <td>XS</td>\n",
       "      <td>JNE3633-KR-XS</td>\n",
       "      <td>1</td>\n",
       "      <td>5.436</td>\n",
       "      <td>No Promotion</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Category Style Size            SKU  Quantity  Amount  \\\n",
       "48972 2022-03-31      Set     J   XS   J0127-SKD-XS         1  14.388   \n",
       "49017 2022-03-31    kurta   JNE   XS  JNE3633-KR-XS         1   5.436   \n",
       "\n",
       "          Promotions  B2B  \n",
       "48972  Free Shipping    0  \n",
       "49017   No Promotion    0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Style</th>\n",
       "      <th>Size</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Promotions</th>\n",
       "      <th>B2B</th>\n",
       "      <th>SKU_Color</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Set</td>\n",
       "      <td>J</td>\n",
       "      <td>XS</td>\n",
       "      <td>1</td>\n",
       "      <td>14.388</td>\n",
       "      <td>Free Shipping</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT LISTED</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kurta</td>\n",
       "      <td>JNE</td>\n",
       "      <td>XS</td>\n",
       "      <td>1</td>\n",
       "      <td>5.436</td>\n",
       "      <td>No Promotion</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT LISTED</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category Style Size  Quantity  Amount     Promotions  B2B   SKU_Color  \\\n",
       "0      Set     J   XS         1  14.388  Free Shipping    0  NOT LISTED   \n",
       "1    kurta   JNE   XS         1   5.436   No Promotion    0  NOT LISTED   \n",
       "\n",
       "   Month  Day  \n",
       "0      3   31  \n",
       "1      3   31  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_engineered = feature_engineering(df_cleaned)\n",
    "df_engineered.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Splitting Dataset</u>\n",
    "- #### Train Test Split my Dataset\n",
    "    - transfer target variable to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84720, 9), (21180, 9))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(df_engineered)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Preprocessing (Encoding & Scaling)</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84720, 59), (21180, 59))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, preprocessor = preprocess_features(X_train.copy(), X_test.copy())\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Addressing Skewness</u>\n",
    "- #### Introducing functions to handle skewness of dataset\n",
    "    \n",
    "    - transforming target variable column and present a more evenly distributed data to analyze\n",
    "    - will need to convert back to original form later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2456782846199202, 0.22925266829601365)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming apply_transformation is your function that applies the best transformation\n",
    "y_train, _ = transform_target(y_train.copy())\n",
    "y_test, _ = transform_target(y_test.copy())\n",
    "y_train.skew(), y_test.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Developing a Baseline Model to Beat (DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Initializing baseline model(DecisionTreeRegressor) & models to beat the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    XGBRegressor(),\n",
    "    LGBMRegressor(force_col_wise=True, verbosity=-1),\n",
    "    GradientBoostingRegressor(),\n",
    "    RandomForestRegressor()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Runnning models against baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring DecisionTreeRegressor\n",
      "Scoring XGBRegressor\n",
      "Scoring LGBMRegressor\n",
      "Scoring GradientBoostingRegressor\n",
      "Scoring RandomForestRegressor\n"
     ]
    }
   ],
   "source": [
    "model_scores = []\n",
    "    \n",
    "for model in models:\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics = evaluate_model(model, \n",
    "                             X_train, \n",
    "                             y_train, \n",
    "                             X_test, \n",
    "                             y_test, \n",
    "                             transformations)\n",
    "    \n",
    "    # Append the scoring with metrics dictionary\n",
    "    model_scores.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE Train Set</th>\n",
       "      <th>RMSE Test Set</th>\n",
       "      <th>MAE Train Set</th>\n",
       "      <th>MAE Test Set</th>\n",
       "      <th>R2 Train Set</th>\n",
       "      <th>R2 Test Set</th>\n",
       "      <th>Time(sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>6.159424</td>\n",
       "      <td>6.178601</td>\n",
       "      <td>5.696946</td>\n",
       "      <td>5.695006</td>\n",
       "      <td>-327.762646</td>\n",
       "      <td>-321.639396</td>\n",
       "      <td>0.219351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>6.059681</td>\n",
       "      <td>6.055635</td>\n",
       "      <td>5.657978</td>\n",
       "      <td>5.654694</td>\n",
       "      <td>-317.201179</td>\n",
       "      <td>-308.924894</td>\n",
       "      <td>0.262181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>6.038056</td>\n",
       "      <td>6.039500</td>\n",
       "      <td>5.649997</td>\n",
       "      <td>5.648124</td>\n",
       "      <td>-314.934167</td>\n",
       "      <td>-307.275445</td>\n",
       "      <td>0.214370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>5.984069</td>\n",
       "      <td>5.980213</td>\n",
       "      <td>5.628486</td>\n",
       "      <td>5.623955</td>\n",
       "      <td>-309.309860</td>\n",
       "      <td>-301.252794</td>\n",
       "      <td>5.714258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>6.132686</td>\n",
       "      <td>6.136255</td>\n",
       "      <td>5.686982</td>\n",
       "      <td>5.679709</td>\n",
       "      <td>-324.914555</td>\n",
       "      <td>-317.231999</td>\n",
       "      <td>13.275930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  RMSE Train Set  RMSE Test Set  MAE Train Set  \\\n",
       "0      DecisionTreeRegressor        6.159424       6.178601       5.696946   \n",
       "1               XGBRegressor        6.059681       6.055635       5.657978   \n",
       "2              LGBMRegressor        6.038056       6.039500       5.649997   \n",
       "3  GradientBoostingRegressor        5.984069       5.980213       5.628486   \n",
       "4      RandomForestRegressor        6.132686       6.136255       5.686982   \n",
       "\n",
       "   MAE Test Set  R2 Train Set  R2 Test Set  Time(sec)  \n",
       "0      5.695006   -327.762646  -321.639396   0.219351  \n",
       "1      5.654694   -317.201179  -308.924894   0.262181  \n",
       "2      5.648124   -314.934167  -307.275445   0.214370  \n",
       "3      5.623955   -309.309860  -301.252794   5.714258  \n",
       "4      5.679709   -324.914555  -317.231999  13.275930  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores_df = pd.DataFrame(model_scores)\n",
    "model_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Grid Search Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Grid Search for the top 2 performing models (LGBMRegressor & XGBRegressor)\n",
    "\n",
    "- The goal is to narrow down the best model to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Scoring Dictionary\n",
    "scoring_dict = {\n",
    "    'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), \n",
    "                        greater_is_better=False),\n",
    "    'MAE': 'neg_mean_absolute_error',\n",
    "    'R2': 'r2'\n",
    "}\n",
    "\n",
    "# Initialize Grid Models\n",
    "grid_models = {\n",
    "    'XGBoost': XGBRegressor(enable_categorical=True),\n",
    "    'LightGBM': LGBMRegressor(force_col_wise=True, verbosity=-1),\n",
    "}\n",
    "\n",
    "# Setting parameters for grid search\n",
    "param_dict = {\n",
    "\n",
    "    'XGBoost': {'n_estimators': [460],\n",
    "                'max_depth': [4, 5]},\n",
    "#                 'learning_rate': [0.1],\n",
    "#                 'reg_alpha': [0.01, 0.1],\n",
    "#                 'reg_lambda': [2.4, 2.5, 2.6]},\n",
    "\n",
    "    'LightGBM': {'num_leaves': [57, 58, 59, 60]},\n",
    "                 'n_estimators': [740, 745, 750],\n",
    "                 'max_depth': [6, 7, 8],\n",
    "#                  'learning_rate': [0.1],\n",
    "#                  'reg_alpha': [0.01, 0.1],\n",
    "#                  'reg_lambda': [0.3, 0.4, 0.5]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for XGBoost...\n",
      "Running GridSearchCV for LightGBM...\n",
      "Best Model: LightGBM with parameters {'num_leaves': 58}\n",
      "The best model is: LGBMRegressor\n"
     ]
    }
   ],
   "source": [
    "best_model, model_performance_df = grid_search_for_best_model(grid_models,\n",
    "                                                              param_dict,\n",
    "                                                              scoring_dict,\n",
    "                                                              X_train, \n",
    "                                                              y_train)\n",
    "print('The best model is:',best_model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Time(sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 460}</td>\n",
       "      <td>0.445243</td>\n",
       "      <td>0.149423</td>\n",
       "      <td>0.659385</td>\n",
       "      <td>8.031399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'num_leaves': 58}</td>\n",
       "      <td>0.444855</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.660566</td>\n",
       "      <td>3.808651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model                        Best Parameters      RMSE       MAE  \\\n",
       "0   XGBoost  {'max_depth': 4, 'n_estimators': 460}  0.445243  0.149423   \n",
       "1  LightGBM                     {'num_leaves': 58}  0.444855  0.148846   \n",
       "\n",
       "         R2  Time(sec)  \n",
       "0  0.659385   8.031399  \n",
       "1  0.660566   3.808651  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Feature Importance</u>\n",
    "#### Percentage Explainer\n",
    "- Aggregating the transformed columns back to percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor - Feature Importance for Category: 12.44%\n",
      "LGBMRegressor - Feature Importance for Style: 11.82%\n",
      "LGBMRegressor - Feature Importance for Size: 17.63%\n",
      "LGBMRegressor - Feature Importance for Promotions: 13.35%\n",
      "LGBMRegressor - Feature Importance for SKU_Color: 1.49%\n",
      "LGBMRegressor - Feature Importance for Quantity: 2.75%\n",
      "LGBMRegressor - Feature Importance for Month: 12.72%\n",
      "LGBMRegressor - Feature Importance for Day: 27.11%\n",
      "LGBMRegressor - Feature Importance for B2B: 0.68%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance Name</th>\n",
       "      <th>Importance(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>scale__Day</td>\n",
       "      <td>27.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>scale__Month</td>\n",
       "      <td>12.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>onehot__Promotions_Free-Financing</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>onehot__Promotions_Free Shipping</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>onehot__Style_J</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>onehot__Promotions_No Promotion</td>\n",
       "      <td>3.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>onehot__Category_Set</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>onehot__Size_S</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>onehot__Size_XS</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>scale__Quantity</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>onehot__Size_3XL</td>\n",
       "      <td>2.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>onehot__Size_XXL</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>onehot__Size_L</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>onehot__Size_M</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>onehot__Category_kurta</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>onehot__Style_JNE</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>onehot__Size_XL</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>onehot__Category_Western Dress</td>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>onehot__Style_SET</td>\n",
       "      <td>1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>onehot__Category_Top</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>onehot__Category_Ethnic Dress</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>onehot__Promotions_Coupon</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onehot__Category_Blouse</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>onehot__Promotions_Duplicated</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>onehot__Style_NW</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>onehot__SKU_Color_NOT LISTED</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>onehot__Style_PJNE</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>remainder__B2B</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>onehot__Style_PSET</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onehot__Category_Bottom</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>onehot__Style_PJ</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>onehot__SKU_Color_BLACK</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onehot__Category_Saree</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>onehot__Style_BL</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>onehot__Style_MEN</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>onehot__Style_SAR</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>onehot__Size_Any</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>onehot__Size_4XL</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>onehot__Style_BTM</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>onehot__SKU_Color_GOLD</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>onehot__SKU_Color_RED</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>onehot__Style_AN</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>onehot__Size_5XL</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>onehot__SKU_Color_CHIKU</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>onehot__SKU_Color_GREEN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>onehot__SKU_Color_MAROON</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>onehot__SKU_Color_MUSTARD</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>onehot__SKU_Color_NAVY</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>onehot__Size_6XL</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>onehot__SKU_Color_ORANGE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>onehot__SKU_Color_PINK</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>onehot__SKU_Color_PURPLE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>onehot__SKU_Color_BLUE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>onehot__SKU_Color_WHITE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>onehot__SKU_Color_YELLOW</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>onehot__SKU_Color_BIEGE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>onehot__SKU_Color_BEIGE</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>onehot__Style_CH</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>onehot__SKU_Color_BROWN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Importance Name  Importance(%)\n",
       "57                         scale__Day          27.11\n",
       "56                       scale__Month          12.72\n",
       "35  onehot__Promotions_Free-Financing           3.93\n",
       "34   onehot__Promotions_Free Shipping           3.88\n",
       "12                    onehot__Style_J           3.86\n",
       "36    onehot__Promotions_No Promotion           3.02\n",
       "4                onehot__Category_Set           2.89\n",
       "28                     onehot__Size_S           2.84\n",
       "30                    onehot__Size_XS           2.79\n",
       "55                    scale__Quantity           2.75\n",
       "21                   onehot__Size_3XL           2.56\n",
       "31                   onehot__Size_XXL           2.32\n",
       "26                     onehot__Size_L           2.26\n",
       "27                     onehot__Size_M           2.26\n",
       "7              onehot__Category_kurta           2.05\n",
       "13                  onehot__Style_JNE           1.98\n",
       "29                    onehot__Size_XL           1.98\n",
       "6      onehot__Category_Western Dress           1.91\n",
       "20                  onehot__Style_SET           1.86\n",
       "5                onehot__Category_Top           1.70\n",
       "2       onehot__Category_Ethnic Dress           1.63\n",
       "32          onehot__Promotions_Coupon           1.54\n",
       "0             onehot__Category_Blouse           1.14\n",
       "33      onehot__Promotions_Duplicated           0.98\n",
       "15                   onehot__Style_NW           0.77\n",
       "48       onehot__SKU_Color_NOT LISTED           0.77\n",
       "17                 onehot__Style_PJNE           0.74\n",
       "58                     remainder__B2B           0.68\n",
       "18                 onehot__Style_PSET           0.63\n",
       "1             onehot__Category_Bottom           0.63\n",
       "16                   onehot__Style_PJ           0.53\n",
       "39            onehot__SKU_Color_BLACK           0.53\n",
       "3              onehot__Category_Saree           0.47\n",
       "9                    onehot__Style_BL           0.46\n",
       "14                  onehot__Style_MEN           0.37\n",
       "19                  onehot__Style_SAR           0.33\n",
       "25                   onehot__Size_Any           0.33\n",
       "22                   onehot__Size_4XL           0.23\n",
       "10                  onehot__Style_BTM           0.23\n",
       "43             onehot__SKU_Color_GOLD           0.11\n",
       "52              onehot__SKU_Color_RED           0.09\n",
       "8                    onehot__Style_AN           0.07\n",
       "23                   onehot__Size_5XL           0.05\n",
       "42            onehot__SKU_Color_CHIKU           0.00\n",
       "44            onehot__SKU_Color_GREEN           0.00\n",
       "45           onehot__SKU_Color_MAROON           0.00\n",
       "46          onehot__SKU_Color_MUSTARD           0.00\n",
       "47             onehot__SKU_Color_NAVY           0.00\n",
       "24                   onehot__Size_6XL           0.00\n",
       "49           onehot__SKU_Color_ORANGE           0.00\n",
       "50             onehot__SKU_Color_PINK           0.00\n",
       "51           onehot__SKU_Color_PURPLE           0.00\n",
       "40             onehot__SKU_Color_BLUE           0.00\n",
       "53            onehot__SKU_Color_WHITE           0.00\n",
       "54           onehot__SKU_Color_YELLOW           0.00\n",
       "38            onehot__SKU_Color_BIEGE           0.00\n",
       "37            onehot__SKU_Color_BEIGE           0.00\n",
       "11                   onehot__Style_CH           0.00\n",
       "41            onehot__SKU_Color_BROWN           0.00"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the best model to understand the feature importance\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize the class with feature names\n",
    "feature_importance_processor = ModelFeatureImportance(feature_names=preprocessor.get_feature_names_out())\n",
    "\n",
    "# Process feature importances for each model\n",
    "feature_importance_processor.process_model(best_model, X_test, y_test)\n",
    "\n",
    "# Display the calculated feature importances\n",
    "feature_importance_processor.display_importances()\n",
    "importances_df = get_importances_expanded(preprocessor, best_model)\n",
    "importances_df.sort_values(by='Importance(%)',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u> Deploy Best Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error(MAPE): 259.1451525023607%\n",
      "RMSE: 6.047657413123387\n",
      "MAE: 5.653042326015045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(105900, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape, rmse, mae, final_output = deploy_best_model(best_model, \n",
    "                                                  X_train, \n",
    "                                                  y_train,\n",
    "                                                  X_test, \n",
    "                                                  y_test)\n",
    "\n",
    "# Display the metrics\n",
    "print(f'Mean Absolute Percentage Error(MAPE): {mape}%')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <u>Plotting the Model's Accuracy Over Time</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
