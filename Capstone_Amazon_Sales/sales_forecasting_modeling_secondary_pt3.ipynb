{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IruZsj1v2ukt"
   },
   "source": [
    "# Sales Forecasting cont. (pt.3)\n",
    "\n",
    "### Modeling with the secondary objective of:\n",
    "**Identify Key Sales Drivers:** \n",
    "- Uncover the factors that significantly impact sales. \n",
    "- By analyzing product categories, sales channels, promotional activities, and customer purchasing patterns, we can pinpoint what influences buyers. \n",
    "- This insight will inform targeted marketing and promotional strategies, boosting sales efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Retrieving Data from pt.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RAtqBlHh2uky"
   },
   "outputs": [],
   "source": [
    "# INSTALLATIONS\n",
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install numpy\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install category_encoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Importing Libraries into Project\n",
    "    - Setting pandas parameters to see the full amount of information from my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Reading file into my project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lmpTpvze2ukz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading File.\n"
     ]
    }
   ],
   "source": [
    "f_path = r'/content/drive/MyDrive/UCBerkeley/SalesForecasting/data/AmazonSalesReport.csv'\n",
    "csv_path = r'data\\AmazonSalesReport.csv'\n",
    "xlsx_path = r'data\\AmazonSalesReport.xlsx'\n",
    "\n",
    "if os.path.isfile(csv_path):\n",
    "    print('Reading File.')\n",
    "else:\n",
    "    print(\"Need to create this file\")\n",
    "    # Load the XLSX file\n",
    "    # df = pd.read_excel(xlsx_path)\n",
    "    # -----------------------------------------\n",
    "    # Save to CSV format\n",
    "    # df.to_csv(csv_path, index=False)  # Set index=False to not write row indices\n",
    "\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Grabbing a sample of my dataset to reference decisions from part 1 of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Status</th>\n",
       "      <th>Fulfilment</th>\n",
       "      <th>Sales Channel</th>\n",
       "      <th>ship-service-level</th>\n",
       "      <th>Style</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Category</th>\n",
       "      <th>Size</th>\n",
       "      <th>ASIN</th>\n",
       "      <th>Courier Status</th>\n",
       "      <th>Qty</th>\n",
       "      <th>currency</th>\n",
       "      <th>Amount</th>\n",
       "      <th>ship-city</th>\n",
       "      <th>ship-state</th>\n",
       "      <th>ship-postal-code</th>\n",
       "      <th>ship-country</th>\n",
       "      <th>promotion-ids</th>\n",
       "      <th>B2B</th>\n",
       "      <th>fulfilled-by</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61408</th>\n",
       "      <td>404-9540793-1177956</td>\n",
       "      <td>2022-05-22</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon.in</td>\n",
       "      <td>Expedited</td>\n",
       "      <td>SET293</td>\n",
       "      <td>SET293-KR-NP-S</td>\n",
       "      <td>Set</td>\n",
       "      <td>S</td>\n",
       "      <td>B09K3G4TS6</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>1</td>\n",
       "      <td>INR</td>\n",
       "      <td>736.0</td>\n",
       "      <td>IMPHAL</td>\n",
       "      <td>MANIPUR</td>\n",
       "      <td>795001.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>IN Core Free Shipping 2015/04/08 23-48-5-108</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Order ID        Date   Status Fulfilment Sales Channel   \\\n",
       "61408  404-9540793-1177956  2022-05-22  Shipped     Amazon      Amazon.in   \n",
       "\n",
       "      ship-service-level   Style             SKU Category Size        ASIN  \\\n",
       "61408          Expedited  SET293  SET293-KR-NP-S      Set    S  B09K3G4TS6   \n",
       "\n",
       "      Courier Status  Qty currency  Amount ship-city ship-state  \\\n",
       "61408        Shipped    1      INR   736.0    IMPHAL    MANIPUR   \n",
       "\n",
       "       ship-postal-code ship-country  \\\n",
       "61408          795001.0           IN   \n",
       "\n",
       "                                      promotion-ids    B2B fulfilled-by  \\\n",
       "61408  IN Core Free Shipping 2015/04/08 23-48-5-108  False          NaN   \n",
       "\n",
       "       Unnamed: 22  \n",
       "61408          0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Addressing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Removing Duplicates with Order ID the Same\n",
    "    - False-Positive Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 6\n",
      "Number of rows Before Drop: 128975\n",
      "Number of rows After Drop: 128969\n"
     ]
    }
   ],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print(\"\\nNumber of duplicate rows:\", duplicates)\n",
    "print(f\"Number of rows Before Drop: {df.shape[0]}\")\n",
    "df = df.copy().drop_duplicates()\n",
    "print(f\"Number of rows After Drop: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Addressing Unnecessary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Only Keeping Necessary Columns for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vr3Qajk5g4o0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Category', 'Style', 'Size', 'SKU', 'Qty', 'currency', 'Amount',\n",
       "       'Sales Channel ', 'promotion-ids', 'B2B'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to keep\n",
    "cols_needed = ['Category', 'Style', 'Size', 'SKU', 'Qty', 'currency', 'Amount', 'Sales Channel ', 'promotion-ids', 'B2B']\n",
    "df = df[cols_needed]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Addressing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Filling missing values in 'promotion-ids' with \"No Promotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3j5azOkbhp_y"
   },
   "outputs": [],
   "source": [
    "df['promotion-ids'].fillna(\"No Promotion\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Drop missing values to disclude any potential biases\n",
    "    - Since 'Amount' is crucial for analysis\n",
    "    - There is only 1 type of currency wich is INR (Indian Rupee) * 0.012 for USD conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4nkXY57GhqP1"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['currency', 'Amount'], inplace=True)\n",
    "df.drop(['currency', 'Sales Channel '], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Confirm changes by displaying remaining missing values, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "du9SNe6q-dBT",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category         0\n",
       "Style            0\n",
       "Size             0\n",
       "SKU              0\n",
       "Qty              0\n",
       "Amount           0\n",
       "promotion-ids    0\n",
       "B2B              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Addressing Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Getting original skew of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7n5bhinf-dEC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skew for Qty: -2.689053634974344\n",
      "Skew for Amount: 0.8855215498451299\n"
     ]
    }
   ],
   "source": [
    "num_cols = df.select_dtypes(exclude='object').columns.to_list()\n",
    "num_cols.remove('B2B')\n",
    "for col in num_cols:\n",
    "    print(f\"Skew for {col}: {df[col].skew()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Introducing functions to handle skewness of dataset\n",
    "    \n",
    "    - transforming target variable column and present a more evenly distributed data to analyze\n",
    "    - will need to convert back to original form later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#\n",
    "#\n",
    "#\n",
    "def cap_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "def transform_feature(data, col):\n",
    "    best_skew=0.5\n",
    "    best_transformation = None\n",
    "    transformed_name = None\n",
    "    transformations = {\n",
    "        \"sqrt\": np.sqrt(data[col]),\n",
    "        \"p1_3\": data[col]**(1/3),\n",
    "        \"p1_4\": data[col]**(1/4),\n",
    "        \"p1_5\": data[col]**(1/5),\n",
    "        \"log\": np.log(data[col] + 1e-10)\n",
    "    }\n",
    "    for k,v in transformations.items():\n",
    "        if abs(v.skew()) < abs(best_skew):\n",
    "            best_transformation = v\n",
    "            transformed_name = f'{col}_{k}'\n",
    "        else:\n",
    "            continue\n",
    "    if best_transformation != None:\n",
    "        data[transformed_name] = best_transformation\n",
    "        print('New feature transformed:',transformed_name)\n",
    "        return data\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Implementing Dictionary for Functions to be Analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_df = []\n",
    "outlier_decisions = {\n",
    "    \"Original\": df,\n",
    "    \"Remove\": remove_outliers(df.copy(), 'Amount'),\n",
    "    \"Capping\": cap_outliers(df.copy(), 'Amount'),\n",
    "    \"Transforming\" : transform_feature(df.copy(), 'Amount')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Putting Functions into a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HANDLING OUTLIERS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Data Reduction (Quantity)</th>\n",
       "      <th>Original Skew</th>\n",
       "      <th>Target Feature Skew</th>\n",
       "      <th>Rows In Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>Qty</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.689054</td>\n",
       "      <td>-2.689054</td>\n",
       "      <td>121177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remove</td>\n",
       "      <td>Qty</td>\n",
       "      <td>2.970861</td>\n",
       "      <td>-2.689054</td>\n",
       "      <td>-3.849247</td>\n",
       "      <td>117577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capping</td>\n",
       "      <td>Qty</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.689054</td>\n",
       "      <td>-2.689054</td>\n",
       "      <td>121177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transforming</td>\n",
       "      <td>Qty</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.689054</td>\n",
       "      <td>-2.689054</td>\n",
       "      <td>121177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Original</td>\n",
       "      <td>Amount</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>121177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Remove</td>\n",
       "      <td>Amount</td>\n",
       "      <td>2.970861</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>0.352470</td>\n",
       "      <td>117577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Capping</td>\n",
       "      <td>Amount</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>0.508087</td>\n",
       "      <td>121177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transforming</td>\n",
       "      <td>Amount</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>121177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Decision Feature  Data Reduction (Quantity)  Original Skew  \\\n",
       "0      Original     Qty                   0.000000      -2.689054   \n",
       "1        Remove     Qty                   2.970861      -2.689054   \n",
       "2       Capping     Qty                   0.000000      -2.689054   \n",
       "3  Transforming     Qty                   0.000000      -2.689054   \n",
       "4      Original  Amount                   0.000000       0.885522   \n",
       "5        Remove  Amount                   2.970861       0.885522   \n",
       "6       Capping  Amount                   0.000000       0.885522   \n",
       "7  Transforming  Amount                   0.000000       0.885522   \n",
       "\n",
       "   Target Feature Skew  Rows In Dataset  \n",
       "0            -2.689054           121177  \n",
       "1            -3.849247           117577  \n",
       "2            -2.689054           121177  \n",
       "3            -2.689054           121177  \n",
       "4             0.885522           121177  \n",
       "5             0.352470           117577  \n",
       "6             0.508087           121177  \n",
       "7             0.885522           121177  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in num_cols:\n",
    "    for k,v in outlier_decisions.items():\n",
    "        decision = v\n",
    "        data_reduction_qty = 100 - (len(v) / len(df) * 100)\n",
    "        decision_df.append({\n",
    "            'Decision': k,\n",
    "            'Feature': col, \n",
    "            'Data Reduction (Quantity)': data_reduction_qty,\n",
    "            'Original Skew': df[col].skew(),\n",
    "            'Target Feature Skew': decision[col].skew(),\n",
    "            'Rows In Dataset': decision.shape[0],\n",
    "        })\n",
    "decision_df = pd.DataFrame(decision_df)\n",
    "print(\"HANDLING OUTLIERS:\")\n",
    "decision_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Deciding how I want to handle outliers(test_decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121177, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = outlier_decisions[\"Original\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Convert boolean True/False feature \"B2B\" to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['B2B'] = df['B2B'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Category', 'Style', 'Size', 'SKU', 'Qty', 'Amount', 'promotion-ids',\n",
       "       'B2B'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category         0\n",
       "Style            0\n",
       "Size             0\n",
       "SKU              0\n",
       "Qty              0\n",
       "Amount           0\n",
       "promotion-ids    0\n",
       "B2B              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Addressing High Cardinality Columns</u>\n",
    "- Anything over 10 columns is considered high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Preparing for Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>Dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Category</td>\n",
       "      <td>9</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Style</td>\n",
       "      <td>1373</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Size</td>\n",
       "      <td>11</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SKU</td>\n",
       "      <td>7157</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>promotion-ids</td>\n",
       "      <td>5788</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Unique Values   Dtype\n",
       "0       Category              9  object\n",
       "1          Style           1373  object\n",
       "2           Size             11  object\n",
       "3            SKU           7157  object\n",
       "4  promotion-ids           5788  object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = df.select_dtypes(include = 'object').columns.to_list()\n",
    "cat_col_df = []\n",
    "for k, unique in df[cat_cols].nunique().items():\n",
    "    cat_col_df.append({\n",
    "        'Name': k,\n",
    "        'Unique Values': unique,\n",
    "        'Dtype': df[k].dtype,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(cat_col_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Extracting only the important promotion details in the 'promotion-ids' column\n",
    "    - This will check each cell; if it contains the specified phrase, it retains the promotion type only, otherwise, it leaves the cell as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "promotion-ids\n",
       "Free Shipping     45856\n",
       "No Promotion      41698\n",
       "Free-Financing    32348\n",
       "Duplicated          921\n",
       "Coupon              354\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['promotion-ids'] = df['promotion-ids'].apply(lambda x: \"Free-Financing\" if \"Free-Financing\" in str(x) else x)\n",
    "df['promotion-ids'] = df['promotion-ids'].apply(lambda x: \"Free Shipping\" if \"Free Shipping\" in str(x) else x)\n",
    "df['promotion-ids'] = df['promotion-ids'].apply(lambda x: \"Duplicated\" if \"Duplicated\" in str(x) else x)\n",
    "df['promotion-ids'] = df['promotion-ids'].apply(lambda x: \"Coupon\" if \"Coupon\" in str(x) else x)\n",
    "\n",
    "df['promotion-ids'].value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Extracting Size value \"Free\" and Changing it to \"Any\"\n",
    "    - \"Free\" is not a size\n",
    "    - Replacing \"Free\" with \"Any\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Size: ['S' '3XL' 'XL' 'L' 'XXL' 'XS' '6XL' 'M' '4XL' '5XL' 'Free']\n",
      "Occurances of 'Free' in 'Size': 356\n",
      "Size ready for Ordinal Encoding\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique values in Size: {df['Size'].unique()}\")\n",
    "print(f\"Occurances of 'Free' in 'Size': {df['Size'].value_counts().get('Free', 0)}\")\n",
    "\n",
    "df['Size'] = df['Size'].apply(lambda x: \"Any\" if \"Free\" in str(x) else x)\n",
    "print(\"Size ready for Ordinal Encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Splitting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Train Test Split my Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['Amount'], axis=1)\n",
    "y = df['Amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96941, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Feature Encoding (Categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Separating High and Low Cardinality Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Cardinality Columns:\n",
      "Category          9\n",
      "Size             11\n",
      "promotion-ids     5\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "High Cardinality Columns:\n",
      "Style    1373\n",
      "SKU      7157\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "low_cardinality_cols = []\n",
    "high_cardinality_cols = []\n",
    "cat_cols = X_train.select_dtypes(include='object').columns.to_list()\n",
    "for k,v in X_train[cat_cols].items():\n",
    "    unique = len(X_train[k].unique())\n",
    "    if unique < 15:\n",
    "        low_cardinality_cols.append(k)\n",
    "    else:\n",
    "        high_cardinality_cols.append(k)\n",
    "        \n",
    "print(f\"Low Cardinality Columns:\\n{df[low_cardinality_cols].nunique()}\")\n",
    "print(f\"\\n\\nHigh Cardinality Columns:\\n{df[high_cardinality_cols].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### One Hot Encoding on features with low cardinality\n",
    "    - encoding features [placeholder] with One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first')\n",
    "ohe_features = low_cardinality_cols\n",
    "ohe_features.remove('Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Ordinal Encoding for \"Size\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categories = [['XS', 'S', 'M', 'L', 'XL', 'XXL', '3XL', '4XL', '5XL', '6XL', 'Any']]\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(categories=categories)\n",
    "ordinal_features = ['Size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Target Encoding on Features with High Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "\n",
    "target_features = high_cardinality_cols\n",
    "target_encoder = TargetEncoder()\n",
    "\n",
    "X_train[target_features] = target_encoder.fit_transform(X_train[target_features], y_train)\n",
    "\n",
    "X_test[target_features] = target_encoder.transform(X_test[target_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Feature Scaling (Numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Separating Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Style', 'SKU', 'Qty', 'B2B']\n",
      "Minimum value of Style: 286.8449096534543\n",
      "Maximum value of Style: 1578.3547145599168\n",
      "\n",
      "Minimum value of SKU: 297.9827797060718\n",
      "Maximum value of SKU: 1452.988194507805\n",
      "\n",
      "Minimum value of Qty: 0\n",
      "Maximum value of Qty: 8\n",
      "\n",
      "Minimum value of B2B: 0\n",
      "Maximum value of B2B: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_cols = X_train.select_dtypes(exclude='object').columns.to_list()\n",
    "print(num_cols)\n",
    "for k,v in X_train[num_cols].items():\n",
    "    print(f\"Minimum value of {k}: {min(v)}\")\n",
    "    print(f\"Maximum value of {k}: {max(v)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Scaling Numerical Features\n",
    "    - Scaling Numeric Features with a MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_features = num_cols\n",
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Preprocessor Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', one_hot_encoder, ohe_features),\n",
    "        ('ordinal', ordinal_encoder, ordinal_features),\n",
    "        ('scaler', min_max_scaler, min_max_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Fit and Transform the Preprocessor for Feature Selection/Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96941, 17)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['onehot__Category_Bottom', 'onehot__Category_Dupatta',\n",
       "       'onehot__Category_Ethnic Dress', 'onehot__Category_Saree',\n",
       "       'onehot__Category_Set', 'onehot__Category_Top',\n",
       "       'onehot__Category_Western Dress', 'onehot__Category_kurta',\n",
       "       'onehot__promotion-ids_Duplicated',\n",
       "       'onehot__promotion-ids_Free Shipping',\n",
       "       'onehot__promotion-ids_Free-Financing',\n",
       "       'onehot__promotion-ids_No Promotion', 'ordinal__Size',\n",
       "       'scaler__Style', 'scaler__SKU', 'scaler__Qty', 'scaler__B2B'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feature_names = preprocessor.get_feature_names_out()\n",
    "new_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <u>Selecting Models to Use</u>\n",
    "#### Goals:\n",
    "- \n",
    "#### Explanation:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'LightGBM': LGBMRegressor(force_col_wise=True),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Ridge': Ridge(),    \n",
    "#     'SVR': SVR()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Cross Validation Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished with RandomForest.\n",
      "\n",
      "Finished with XGBoost.\n",
      "[LightGBM] [Info] Total Bins 549\n",
      "[LightGBM] [Info] Number of data points in the train set: 77552, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 649.789985\n",
      "[LightGBM] [Info] Total Bins 548\n",
      "[LightGBM] [Info] Number of data points in the train set: 77553, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 648.301711\n",
      "[LightGBM] [Info] Total Bins 547\n",
      "[LightGBM] [Info] Number of data points in the train set: 77553, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 649.812126\n",
      "[LightGBM] [Info] Total Bins 548\n",
      "[LightGBM] [Info] Number of data points in the train set: 77553, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 648.927033\n",
      "[LightGBM] [Info] Total Bins 549\n",
      "[LightGBM] [Info] Number of data points in the train set: 77553, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 649.122472\n",
      "\n",
      "Finished with LightGBM.\n",
      "\n",
      "Finished with LinearRegression.\n",
      "\n",
      "Finished with GradientBoosting.\n",
      "\n",
      "Finished with Lasso.\n",
      "\n",
      "Finished with Ridge.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring_metrics = {\n",
    "    'mse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error',\n",
    "    'r2': 'r2'\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "algo_score_dict = []\n",
    "\n",
    "# Loop over each model in the model dictionary\n",
    "for model_name, model in model_dict.items():\n",
    "    # Create a pipeline with preprocessing and the current model\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Use cross_validate to evaluate the model\n",
    "    cv_results = cross_validate(pipe, \n",
    "                                X_train, \n",
    "                                y_train, \n",
    "                                cv=5, \n",
    "                                n_jobs=1,\n",
    "                                verbose=0,\n",
    "                                scoring=scoring_metrics,\n",
    "                                return_train_score=False,\n",
    "                                )\n",
    "    \n",
    "    # Calculate the mean of the scores and times for each metric\n",
    "    for metric in scoring_metrics.keys():\n",
    "        mean_score = np.mean(cv_results[f'test_{metric}'])\n",
    "        fit_time = np.mean(cv_results['fit_time'])\n",
    "        score_time = np.mean(cv_results['score_time'])\n",
    "        \n",
    "        # Append the results to the algo_score_dict list\n",
    "        algo_score_dict.append({\n",
    "            'Model Name': model_name,\n",
    "            'Scoring Method': metric,\n",
    "            'Mean CV Score': mean_score,\n",
    "            'Mean Fit Time (seconds)': fit_time,\n",
    "            'Mean Score Time (seconds)': score_time\n",
    "        })\n",
    "    print(f\"\\nFinished with {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Scoring Method</th>\n",
       "      <th>Mean CV Score</th>\n",
       "      <th>Mean Fit Time (seconds)</th>\n",
       "      <th>Mean Score Time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>mse</td>\n",
       "      <td>-14493.279776</td>\n",
       "      <td>12.760801</td>\n",
       "      <td>0.337192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>mae</td>\n",
       "      <td>-49.614554</td>\n",
       "      <td>12.760801</td>\n",
       "      <td>0.337192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.817260</td>\n",
       "      <td>12.760801</td>\n",
       "      <td>0.337192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>mse</td>\n",
       "      <td>-13801.987699</td>\n",
       "      <td>0.195942</td>\n",
       "      <td>0.026808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>mae</td>\n",
       "      <td>-52.004429</td>\n",
       "      <td>0.195942</td>\n",
       "      <td>0.026808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.825962</td>\n",
       "      <td>0.195942</td>\n",
       "      <td>0.026808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>mse</td>\n",
       "      <td>-13942.970361</td>\n",
       "      <td>0.199890</td>\n",
       "      <td>0.027085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>mae</td>\n",
       "      <td>-52.818296</td>\n",
       "      <td>0.199890</td>\n",
       "      <td>0.027085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.199890</td>\n",
       "      <td>0.027085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>mse</td>\n",
       "      <td>-16514.424553</td>\n",
       "      <td>0.069108</td>\n",
       "      <td>0.017153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>mae</td>\n",
       "      <td>-56.981012</td>\n",
       "      <td>0.069108</td>\n",
       "      <td>0.017153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.791817</td>\n",
       "      <td>0.069108</td>\n",
       "      <td>0.017153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>mse</td>\n",
       "      <td>-14284.555244</td>\n",
       "      <td>3.799837</td>\n",
       "      <td>0.031406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>mae</td>\n",
       "      <td>-53.921290</td>\n",
       "      <td>3.799837</td>\n",
       "      <td>0.031406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.819895</td>\n",
       "      <td>3.799837</td>\n",
       "      <td>0.031406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>mse</td>\n",
       "      <td>-17074.044171</td>\n",
       "      <td>0.063545</td>\n",
       "      <td>0.017034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>mae</td>\n",
       "      <td>-57.413988</td>\n",
       "      <td>0.063545</td>\n",
       "      <td>0.017034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.784774</td>\n",
       "      <td>0.063545</td>\n",
       "      <td>0.017034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>mse</td>\n",
       "      <td>-16514.278410</td>\n",
       "      <td>0.067197</td>\n",
       "      <td>0.014867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>mae</td>\n",
       "      <td>-56.964544</td>\n",
       "      <td>0.067197</td>\n",
       "      <td>0.014867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>r2</td>\n",
       "      <td>0.791819</td>\n",
       "      <td>0.067197</td>\n",
       "      <td>0.014867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Name Scoring Method  Mean CV Score  Mean Fit Time (seconds)  \\\n",
       "0       RandomForest            mse  -14493.279776                12.760801   \n",
       "1       RandomForest            mae     -49.614554                12.760801   \n",
       "2       RandomForest             r2       0.817260                12.760801   \n",
       "3            XGBoost            mse  -13801.987699                 0.195942   \n",
       "4            XGBoost            mae     -52.004429                 0.195942   \n",
       "5            XGBoost             r2       0.825962                 0.195942   \n",
       "6           LightGBM            mse  -13942.970361                 0.199890   \n",
       "7           LightGBM            mae     -52.818296                 0.199890   \n",
       "8           LightGBM             r2       0.824200                 0.199890   \n",
       "9   LinearRegression            mse  -16514.424553                 0.069108   \n",
       "10  LinearRegression            mae     -56.981012                 0.069108   \n",
       "11  LinearRegression             r2       0.791817                 0.069108   \n",
       "12  GradientBoosting            mse  -14284.555244                 3.799837   \n",
       "13  GradientBoosting            mae     -53.921290                 3.799837   \n",
       "14  GradientBoosting             r2       0.819895                 3.799837   \n",
       "15             Lasso            mse  -17074.044171                 0.063545   \n",
       "16             Lasso            mae     -57.413988                 0.063545   \n",
       "17             Lasso             r2       0.784774                 0.063545   \n",
       "18             Ridge            mse  -16514.278410                 0.067197   \n",
       "19             Ridge            mae     -56.964544                 0.067197   \n",
       "20             Ridge             r2       0.791819                 0.067197   \n",
       "\n",
       "    Mean Score Time (seconds)  \n",
       "0                    0.337192  \n",
       "1                    0.337192  \n",
       "2                    0.337192  \n",
       "3                    0.026808  \n",
       "4                    0.026808  \n",
       "5                    0.026808  \n",
       "6                    0.027085  \n",
       "7                    0.027085  \n",
       "8                    0.027085  \n",
       "9                    0.017153  \n",
       "10                   0.017153  \n",
       "11                   0.017153  \n",
       "12                   0.031406  \n",
       "13                   0.031406  \n",
       "14                   0.031406  \n",
       "15                   0.017034  \n",
       "16                   0.017034  \n",
       "17                   0.017034  \n",
       "18                   0.014867  \n",
       "19                   0.014867  \n",
       "20                   0.014867  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(algo_score_dict)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Grid Search with Cross Validation</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007407407407407406"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
